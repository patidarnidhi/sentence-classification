{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8c1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from torch import nn\n",
    "# from torchnlp.datasets import imdb_dataset      # --> We are using our own uploaded dataset.\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd12dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.seed(321)\n",
    "np.random.seed(321)\n",
    "torch.manual_seed(321)\n",
    "torch.cuda.manual_seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8da95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e945717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "77091774",
   "metadata": {},
   "source": [
    "#############################################Prepare the data#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3beab021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_file_path = 'dataset/train.csv.zip'\n",
    "zip_file_path = 'dataset/IMDB Dataset.csv.zip'\n",
    "target_directory = 'dataset/'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad0f42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('dataset/train.csv')\n",
    "# print(train_data['sentiment'].isna().any())\n",
    "# print(train_data['text'].isna().any())\n",
    "# changing positive and negative into numeric values\n",
    "\n",
    "def cat2num(value):\n",
    "    if value=='positive': \n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "# df=pd.read_csv(\"dataset/train.csv\")\n",
    "df = pd.read_csv(\"dataset/IMDB Dataset.csv\")\n",
    "df['sentiment']  =  df['sentiment'].apply(cat2num)\n",
    "train = df[:45000]\n",
    "test = df[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97d7103f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 2)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "train.head(5)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddb96f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train_data[:20000]\n",
    "# test = train_data[20000:]\n",
    "# train['sentiment'] = train['sentiment'].map({'positive' : 0, 'negative' : 1})\n",
    "# test['sentiment']=test['sentiment'].map({'positive' : 0, 'negative' : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39c8fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32575</th>\n",
       "      <td>I've been trying to write a plot summary for s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "32575  I've been trying to write a plot summary for s...          0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a26821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d5f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "896211ca",
   "metadata": {},
   "source": [
    "##########################################Intialize model and tokenizer###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "527dd33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (4.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: requests in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: filelock in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: fsspec in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e66a53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65235a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf3e9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e46640e3",
   "metadata": {},
   "source": [
    "########################################Preprocess the IMDB data #################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22f183f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'this', 'ka', '##ggle', 'notebook', ',', 'i', 'will', 'do', 'sentiment', 'analysis', 'using', 'bert', 'with', 'hugging', '##face']\n",
      "[1999, 2023, 10556, 24679, 14960, 1010, 1045, 2097, 2079, 15792, 4106, 2478, 14324, 2007, 17662, 12172]\n"
     ]
    }
   ],
   "source": [
    "# But first see BERT tokenizer exmaples and other required stuff!\n",
    "\n",
    "example='In this Kaggle notebook, I will do sentiment analysis using BERT with Huggingface'\n",
    "tokens=tokenizer.tokenize(example)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01939c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will accept our train and convert each row into an InputExample object.\n",
    "\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "def convert_data_to_examples(train, test, review, sentiment): \n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[review], \n",
    "                                                          label = x[sentiment]), axis = 1)\n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[review], \n",
    "                                                          label = x[sentiment]), axis = 1,)\n",
    "  \n",
    "    return train_InputExamples, validation_InputExamples\n",
    "\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'review',  'sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f5cfc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will tokenize the InputExample objects, then create the required input format with the tokenized \n",
    "#objects, finally, create an input dataset that we can feed to the model.\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in tqdm(examples):\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n",
    "            max_length=max_length,    # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'review'\n",
    "LABEL_COLUMN = 'sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "faeb0f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/45000 [00:00<?, ?it/s]/data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████| 45000/45000 [06:02<00:00, 124.22it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08365e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5000/5000 [00:39<00:00, 127.62it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "784ae6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2814/2814 [==============================] - 10218s 4s/step - loss: 0.2423 - accuracy: 0.8987 - val_loss: 0.2767 - val_accuracy: 0.8892\n",
      "Epoch 2/2\n",
      "2814/2814 [==============================] - 8116s 3s/step - loss: 0.0667 - accuracy: 0.9770 - val_loss: 0.4138 - val_accuracy: 0.8910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2258124160>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25506295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"output_model\")\n",
    "model.save_pretrained(\"output_model3\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0548129c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0263ab4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "759700f4",
   "metadata": {},
   "source": [
    "#######################################Finetune the model on amazon data#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abe221bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126676</th>\n",
       "      <td>1</td>\n",
       "      <td>This one is super stable because it is running...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_index                                        review_text\n",
       "126676            1  This one is super stable because it is running..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('dataset/test.csv')\n",
    "test['class_index'] = test['class_index'].replace({1: 0, 2: 1})\n",
    "test.drop('review_title', axis=1, inplace=True)\n",
    "test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "061eaad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# finetune = test[25000:25100]\n",
    "test = test[:25000]\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae8e0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will accept our train and convert each row into an InputExample object.\n",
    "\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "def convert_data_to_examples(test, review, sentiment): \n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[review], \n",
    "                                                          label = x[sentiment]), axis = 1,)\n",
    "  \n",
    "    return validation_InputExamples\n",
    "\n",
    "validation_InputExamples = convert_data_to_examples(test, 'review_text',  'class_index')\n",
    "# finetune_InputExamples = convert_data_to_examples(finetune, 'review_text',  'class_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7e9c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/25000 [00:00<?, ?it/s]/data2/home/nidhipatidar/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████| 25000/25000 [00:31<00:00, 803.45it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "783b4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune_data = convert_examples_to_tf_dataset(list(finetune_InputExamples), tokenizer)\n",
    "# finetune_data = finetune_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fc9acf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at output_model were not used when initializing TFBertForSequenceClassification: ['dropout_227']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at output_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = TFBertForSequenceClassification.from_pretrained(\"output_model\")\n",
    "loaded_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eadf0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model.fit(finetune_data, epochs=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fefbbe3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "018d73bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b1f90e68",
   "metadata": {},
   "source": [
    "########################################Test the model on amazon data#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "555cd2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 690s 879ms/step - loss: 6.1830 - accuracy: 0.4863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 6.183012008666992, 'accuracy': 0.48627999424934387}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded_model = TFBertForSequenceClassification.from_pretrained(\"output_model\")\n",
    "result = loaded_model.evaluate(validation_data)\n",
    "dict(zip(loaded_model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbadf78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648e228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
